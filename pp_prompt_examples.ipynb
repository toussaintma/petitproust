{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7iJGCuAh7DxDghOcHYtv6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toussaintma/petitproust/blob/main/pp_prompt_examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CL5ti57lB7Xz"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "IQsJUWwqCC1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from transformers import pipeline\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "text_generation_pipeline = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    temperature=0.2,\n",
        "    do_sample=True,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=True,\n",
        "    max_new_tokens=500,\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
      ],
      "metadata": {
        "id": "1sAoDKvDCF1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"\n",
        "<|system|>\n",
        "\n",
        "You are an expert writer writing in french. Rewrite this text in the litterary style of author Marcel Proust. Write exclusively in french. Use the following example texts of his writing to help:\n",
        "\n",
        "{examples}\n",
        "\n",
        "</s>\n",
        "<|user|>\n",
        "\n",
        "{source}\n",
        "\n",
        "</s>\n",
        "<|assistant|>\n",
        "\n",
        " \"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"examples\", \"source\"],\n",
        "    template=prompt_template,\n",
        ")\n",
        "\n",
        "llm_chain = prompt | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "y45Pd130CRwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_text = \"\"\"\n",
        "Chaque soir je m'endors en rêvant de fusée. Dans mes rêves je suis un cosmonaute dans une fusée partant pour la lune et je rêve.\n",
        "\"\"\"\n",
        "answer_llmonly = llm_chain.invoke({\"examples\": \"\", \"source\": original_text})"
      ],
      "metadata": {
        "id": "xyWmA91UCOrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer_llmonly)"
      ],
      "metadata": {
        "id": "KQgh1KhJCoDp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}